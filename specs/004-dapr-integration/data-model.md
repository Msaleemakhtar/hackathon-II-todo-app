# Data Model: Dapr Integration for Event-Driven Architecture

**Feature**: 004-dapr-integration
**Date**: 2026-01-15
**Phase**: 1 - Design

## Overview

This feature is an **infrastructure migration** and does not introduce new application data models. All existing data models from Feature 001 (Advanced Features) and Feature 002 (Event-Driven) remain unchanged. However, Dapr introduces infrastructure-level data structures for state management and component configuration.

---

## Application Data Models (Unchanged)

### Existing Models (Reference Only)

These models were defined in Feature 001 and remain unchanged:

**Task Model** (`tasks_phaseiii` table):
```python
class Task(SQLModel, table=True):
    __tablename__ = "tasks_phaseiii"

    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: int = Field(foreign_key="users_phaseiii.id", index=True)
    title: str = Field(max_length=255)
    description: Optional[str] = None
    completed: bool = Field(default=False)
    priority: str = Field(default="medium")  # low, medium, high, urgent
    due_date: Optional[datetime] = None
    category_id: Optional[int] = Field(default=None, foreign_key="categories.id")
    recurrence_rule: Optional[str] = None  # iCal RRULE format
    reminder_sent: bool = Field(default=False)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
```

**Category Model** (`categories` table):
```python
class Category(SQLModel, table=True):
    __tablename__ = "categories"

    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: int = Field(foreign_key="users_phaseiii.id", index=True)
    name: str = Field(max_length=100)
    color: Optional[str] = Field(max_length=7, default="#3b82f6")  # Hex color
```

**Tag Model** (`tags_phasev` table):
```python
class Tag(SQLModel, table=True):
    __tablename__ = "tags_phasev"

    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: int = Field(foreign_key="users_phaseiii.id", index=True)
    name: str = Field(max_length=50, unique=True)
```

**Task-Tag Junction** (`task_tags` table):
```python
class TaskTag(SQLModel, table=True):
    __tablename__ = "task_tags"

    task_id: int = Field(foreign_key="tasks_phaseiii.id", primary_key=True)
    tag_id: int = Field(foreign_key="tags_phasev.id", primary_key=True)
```

**No schema changes required** for this feature. All database migrations are complete.

---

## Event Schemas (Unchanged)

Event schemas defined in Feature 002 remain unchanged. Dapr Pub/Sub will wrap these events in CloudEvents format but the `data` field preserves the original schema.

### TaskCreatedEvent
```python
class TaskCreatedEvent(BaseModel):
    event_type: Literal["created"] = "created"
    task_id: int
    task_data: Dict[str, Any]  # Full task dict
    user_id: int
    timestamp: datetime
```

### TaskUpdatedEvent
```python
class TaskUpdatedEvent(BaseModel):
    event_type: Literal["updated"] = "updated"
    task_id: int
    task_data: Dict[str, Any]  # Updated task dict
    changes: Dict[str, Any]  # Changed fields only
    user_id: int
    timestamp: datetime
```

### TaskCompletedEvent
```python
class TaskCompletedEvent(BaseModel):
    event_type: Literal["completed"] = "completed"
    task_id: int
    task_data: Dict[str, Any]
    user_id: int
    timestamp: datetime
    has_recurrence: bool  # True if recurrence_rule is set
```

### TaskDeletedEvent
```python
class TaskDeletedEvent(BaseModel):
    event_type: Literal["deleted"] = "deleted"
    task_id: int
    user_id: int
    timestamp: datetime
```

### ReminderEvent
```python
class ReminderEvent(BaseModel):
    task_id: int
    title: str
    due_at: datetime
    remind_at: datetime
    user_id: int
```

---

## Dapr Infrastructure Data Models

### 1. CloudEvent Envelope (Dapr-Generated)

Dapr automatically wraps all published events in CloudEvents 1.0 format:

```python
class CloudEvent(BaseModel):
    """
    CloudEvents 1.0 specification envelope.
    Generated by Dapr Pub/Sub, not by application code.
    """
    specversion: str = "1.0"
    id: str  # UUID generated by Dapr
    source: str  # App ID (e.g., "backend")
    type: str  # Event type (e.g., "TaskCreatedEvent")
    time: datetime  # ISO 8601 timestamp
    datacontenttype: str = "application/json"
    data: Dict[str, Any]  # Original event schema (TaskCreatedEvent, etc.)
```

**Consumer Handling**:
```python
@router.post("/events/task-created")
async def handle_task_created(request: Request):
    cloud_event = await request.json()  # Receives CloudEvent
    task_data = cloud_event["data"]  # Extract original event
    # Process task_data using existing logic
```

**Relationship**: CloudEvent wraps existing event schemas. Application code publishes original events; Dapr adds CloudEvent metadata.

---

### 2. Dapr State Store Entries

Dapr State Store uses a key-value model stored in PostgreSQL.

**State Entry Model** (Conceptual - managed by Dapr):
```python
class DaprStateEntry(BaseModel):
    """
    Conceptual model for Dapr State Store entries.
    Stored in dapr_state_store table (created by Dapr).
    """
    key: str  # Unique state key (e.g., "notification:last_poll")
    value: Dict[str, Any]  # JSON value
    etag: str  # Optimistic concurrency control
    update_time: datetime
    expiration_time: Optional[datetime]  # TTL support
```

**Database Schema** (Auto-created by Dapr):
```sql
CREATE TABLE dapr_state_store (
    key TEXT PRIMARY KEY,
    value JSONB NOT NULL,
    etag VARCHAR(50) NOT NULL,
    update_time TIMESTAMPTZ DEFAULT NOW(),
    expiration_time TIMESTAMPTZ
);

CREATE INDEX idx_dapr_state_expiration ON dapr_state_store(expiration_time)
WHERE expiration_time IS NOT NULL;
```

**Usage Examples**:

**Notification Service - Last Poll Timestamp**:
```python
# Save state
await dapr_client.save_state(
    store_name="statestore-postgres",
    key="notification:last_poll",
    value={"timestamp": datetime.utcnow().isoformat()},
    metadata={"ttl": "3600"}  # Auto-expire after 1 hour
)

# Retrieve state
state = await dapr_client.get_state(
    store_name="statestore-postgres",
    key="notification:last_poll"
)
last_poll = datetime.fromisoformat(state["timestamp"])
```

**Idempotency Tracking**:
```python
# Check if task reminder was already sent
state_key = f"reminder:task:{task_id}"
existing = await dapr_client.get_state(
    store_name="statestore-postgres",
    key=state_key
)

if existing:
    # Already processed, skip
    return

# Mark as processed
await dapr_client.save_state(
    store_name="statestore-postgres",
    key=state_key,
    value={"processed_at": datetime.utcnow().isoformat(), "user_id": user_id},
    metadata={"ttl": "86400"}  # Expire after 24 hours
)
```

**Relationship**: State Store entries are separate from application database. Used for infrastructure state (polling timestamps, idempotency) rather than business data.

---

### 3. Dapr Job Definition (Jobs API)

Dapr Jobs API uses a scheduled job model.

**Job Definition Model** (Conceptual - managed by Dapr):
```python
class DaprJob(BaseModel):
    """
    Conceptual model for Dapr Jobs API scheduled jobs.
    """
    name: str  # Job name (e.g., "check-due-tasks")
    schedule: str  # Cron expression or @every syntax
    repeats: int  # -1 for infinite
    dueTime: str  # ISO 8601 duration (e.g., "0s" for immediate)
    ttl: str  # Job expiration (e.g., "3600s")
    data: Dict[str, Any]  # Payload passed to callback endpoint
```

**Job Registration** (Application Code):
```python
# Register notification job at startup
async def register_notification_job():
    await httpx.post(
        "http://localhost:3500/v1.0-alpha1/jobs/check-due-tasks",
        json={
            "schedule": "@every 5s",
            "repeats": -1,
            "dueTime": "0s",
            "ttl": "3600s",
            "data": {"type": "notification_check"}
        }
    )
```

**Job Callback Endpoint**:
```python
@router.post("/jobs/check-due-tasks")
async def check_due_tasks_job(request: Request):
    """
    Dapr Scheduler invokes this endpoint every 5 seconds.
    """
    job_data = await request.json()  # Receives job.data payload
    # Execute notification logic
    tasks = await get_tasks_due_soon()
    for task in tasks:
        await publish_reminder_event(task)
    return {"status": "SUCCESS"}
```

**Relationship**: Jobs API replaces the current async polling loop in notification service. Jobs are persisted in Dapr's scheduler backend (etcd) and survive restarts.

---

## Component Configuration Models

Dapr components are defined in Kubernetes YAML (not Python models).

### Pub/Sub Component (Kafka)
```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub-kafka
  namespace: todo-phasev
spec:
  type: pubsub.kafka
  version: v1
  metadata:
    - name: brokers
      value: "bootstrap.redpanda.cloud:9092"
    - name: authType
      value: "sasl"
    - name: saslMechanism
      value: "SCRAM-SHA-256"
    - name: saslUsername
      secretKeyRef:
        name: kafka-secrets
        key: username
    - name: saslPassword
      secretKeyRef:
        name: kafka-secrets
        key: password
  scopes:
    - backend
    - notification-service
```

### State Store Component (PostgreSQL)
```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: statestore-postgres
  namespace: todo-phasev
spec:
  type: state.postgresql
  version: v2
  metadata:
    - name: connectionString
      secretKeyRef:
        name: todo-app-secrets
        key: POSTGRES_CONNECTION_STRING
    - name: tableName
      value: "dapr_state_store"
    - name: cleanupIntervalInSeconds
      value: "3600"
  scopes:
    - backend
    - notification-service
```

### Secrets Store Component
```yaml
apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: kubernetes-secrets
  namespace: todo-phasev
spec:
  type: secretstores.kubernetes
  version: v1
```

---

## Data Flow Diagrams

### Event Publishing Flow (Pre-Migration vs Post-Migration)

**Before (aiokafka)**:
```
MCP Tool → aiokafka Producer → Kafka Broker
```

**After (Dapr)**:
```
MCP Tool → Dapr HTTP API → Dapr Sidecar → Kafka Broker
           (localhost:3500)  (CloudEvents wrap)
```

### Event Consumption Flow (Pre-Migration vs Post-Migration)

**Before (aiokafka)**:
```
Kafka Broker → aiokafka Consumer → Consumer Service Handler
```

**After (Dapr)**:
```
Kafka Broker → Dapr Sidecar → Dapr Subscription → HTTP POST to /events/... → Consumer Service Handler
               (CloudEvents)   (declarative YAML)
```

---

## State Transitions

### Task Reminder State Transition

```
Task Created (with due_date)
  ↓
ReminderEvent published to task-reminders topic
  ↓
Dapr Pub/Sub → Notification Service /events/reminder-sent
  ↓
Notification Service checks reminder_sent flag
  ↓
If reminder_sent = False:
  - Send email via SMTP
  - Update reminder_sent = True in database
  - Save state in Dapr State Store (idempotency)
  ↓
If reminder_sent = True:
  - Skip (already processed)
```

### Recurring Task State Transition

```
Task Completed (with recurrence_rule)
  ↓
TaskCompletedEvent published to task-recurrence topic
  ↓
Dapr Pub/Sub → Recurring Task Service /events/task-completed
  ↓
Parse recurrence_rule (iCal RRULE)
  ↓
Calculate next occurrence date
  ↓
Create new task with same attributes, updated due_date
  ↓
Publish TaskCreatedEvent for new task instance
```

---

## Validation Rules

### CloudEvent Validation (Consumer Side)

```python
def validate_cloud_event(cloud_event: Dict[str, Any]) -> None:
    """Validate CloudEvent structure before processing"""
    assert cloud_event.get("specversion") == "1.0", "Invalid CloudEvents version"
    assert "id" in cloud_event, "Missing event ID"
    assert "source" in cloud_event, "Missing event source"
    assert "type" in cloud_event, "Missing event type"
    assert "data" in cloud_event, "Missing event data"
```

### State Key Validation

```python
def validate_state_key(key: str) -> None:
    """Validate state key format"""
    assert len(key) <= 255, "State key too long"
    assert ":" in key, "State key must follow namespace:identifier format"
    assert not key.startswith("dapr:"), "Reserved namespace"
```

---

## Database Impact

### New Tables (Created by Dapr)

**dapr_state_store**:
- Owner: Dapr State Store component
- Purpose: Key-value state storage with TTL
- Records: ~100-1000 (transient state, auto-cleanup)

**dapr_state_metadata**:
- Owner: Dapr State Store component
- Purpose: Internal Dapr metadata
- Records: ~10 (fixed size)

### Existing Tables (Unchanged)

All application tables remain unchanged:
- `tasks_phaseiii`
- `categories`
- `tags_phasev`
- `task_tags`

**No migrations required** for this feature.

---

## Summary

**Data Model Changes**: None for application data. Dapr introduces infrastructure-level data models:
1. CloudEvent envelope (Dapr-generated, wraps existing events)
2. State Store entries (Dapr-managed, key-value storage)
3. Job definitions (Dapr-managed, scheduled execution)

**Schema Impact**: Dapr creates 2 new tables (`dapr_state_store`, `dapr_state_metadata`) separate from application schema.

**Backward Compatibility**: All existing event schemas remain unchanged. CloudEvents is additive (wraps existing data).

**Next Steps**: Generate API contracts for Dapr HTTP API interactions.
